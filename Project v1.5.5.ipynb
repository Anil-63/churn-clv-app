import sys
import subprocess
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score, classification_report,
    mean_squared_error, r2_score,
    mean_absolute_error, mean_absolute_percentage_error, median_absolute_error)
from xgboost import XGBRegressor
import lightgbm as lgb
from prophet import Prophet
import dask.dataframe as dd
import joblib


datapath =("/Users/anilkumar/Desktop/capestone/E-commerce_Dataset.csv")

df=pd.read_csv(datapath,parse_dates=[['Order_Date', 'Time']])

df.rename(columns={'Order_Date_Time': 'OrderTimestamp'}, inplace=True)
df.tail()

df.info()

df.describe()

df['Order_Day'] = df['OrderTimestamp'].dt.day_name()
df.head()

plt.figure(figsize=(8, 4))
sns.countplot(data=df, x='Order_Day', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
plt.title("Count of Orders by Day of the Week")
plt.xlabel("Day of Week")
plt.ylabel("Number of Orders")
plt.xticks(rotation=45)
plt.show()


customer_df = customer_df.copy()


customer_df['R_Score'] = pd.qcut(customer_df['recency'], 5, labels=[5, 4, 3, 2, 1])
customer_df['F_Score'] = pd.qcut(customer_df['frequency'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5])
customer_df['M_Score'] = pd.qcut(customer_df['monetary'], 5, labels=[1, 2, 3, 4, 5])


customer_df['RFM_Score'] = (
    customer_df['R_Score'].astype(str) +
    customer_df['F_Score'].astype(str) +
    customer_df['M_Score'].astype(str)
)


def assign_segment(rfm):
    if rfm == '555':
        return 'Champions'
    elif rfm[0] in ['4', '5'] and rfm[1] in ['4', '5']:
        return 'Loyal Customers'
    elif rfm[0] in ['1', '2'] and rfm[1] in ['3', '4', '5']:
        return 'At-Risk'
    elif rfm[0] in ['4', '5'] and rfm[1] in ['1', '2']:
        return 'Potential Loyalists'
    elif rfm == '111':
        return 'Low-Value'
    else:
        return 'Others'


customer_df['Segment'] = customer_df['RFM_Score'].apply(assign_segment)


print(customer_df[['Customer_Id', 'RFM_Score', 'Segment']].head())


plt.figure(figsize=(8, 4))
sns.histplot(df['Sales'], bins=30,kde=True)
plt.title("Distribution of Sales Amount")
plt.xlabel("Sales")
plt.ylabel("Frequency")
plt.show()

# Ensure datetime column is clean
df['OrderTimestamp'] = pd.to_datetime(df['OrderTimestamp'], errors='coerce')
df = df[df['OrderTimestamp'].notna()]

# features
df['Order_Day'] = df['OrderTimestamp'].dt.day_name()
df['Order_Hour'] = df['OrderTimestamp'].dt.hour

# Distributions
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
day_distribution = df['Order_Day'].value_counts(normalize=True).reindex(day_order).fillna(0) * 100
hour_distribution = df['Order_Hour'].value_counts(normalize=True).sort_index().fillna(0) * 100

#  peaks
peak_day = day_distribution.idxmax()
peak_day_value = day_distribution[peak_day]
peak_hour = hour_distribution.idxmax()
peak_hour_value = hour_distribution[peak_hour]


fig, axs = plt.subplots(2, 1, figsize=(10, 8))

# 1. Bar plot (Day of Week)
sns.barplot(x=day_distribution.index, y=day_distribution.values, ax=axs[0], palette="Blues_d")
axs[0].set_title("Purchase Distribution by Day of Week (%)")
axs[0].set_ylabel("Percentage of Orders")
axs[0].set_xlabel("Day of Week")
axs[0].tick_params(axis='x', rotation=45)


peak_day_index = day_order.index(peak_day)
axs[0].annotate(f"Peak: {peak_day}",
                xy=(peak_day_index, peak_day_value),
                xytext=(peak_day_index, peak_day_value + 2),
                ha='center',
                arrowprops=dict(arrowstyle='->', color='black'),
                fontsize=9)

#  Line plot (Hour of Day)
sns.lineplot(x=hour_distribution.index, y=hour_distribution.values, ax=axs[1], marker='o', color='mediumpurple')
axs[1].fill_between(hour_distribution.index, hour_distribution.values, alpha=0.2, color='mediumpurple')
axs[1].set_title("Purchase Distribution by Hour of Day (%)")
axs[1].set_ylabel("Percentage of Orders")
axs[1].set_xlabel("Hour of Day (24h)")

axs[1].annotate(f"Peak: {peak_hour}:00",
                xy=(peak_hour, peak_hour_value),
                xytext=(peak_hour + 1, peak_hour_value + 1),
                arrowprops=dict(arrowstyle='->', color='black'),
                fontsize=9)

plt.tight_layout()
plt.show()


customer_df['R_Score'] = pd.qcut(customer_df['recency'], 5, labels=[5, 4, 3, 2, 1])
customer_df['F_Score'] = pd.qcut(customer_df['frequency'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5])
customer_df['M_Score'] = pd.qcut(customer_df['monetary'], 5, labels=[1, 2, 3, 4, 5])


customer_df['RFM_Score'] = (
    customer_df['R_Score'].astype(str) +
    customer_df['F_Score'].astype(str) +
    customer_df['M_Score'].astype(str)
)


def assign_segment(rfm):
    if rfm[0] in ['4', '5'] and rfm[1] in ['4', '5'] and rfm[2] in ['4', '5']:
        return 'Champions'
    elif rfm[0] in ['3', '4'] and rfm[1] in ['4', '5'] and rfm[2] in ['3', '4', '5']:
        return 'Loyal Customers'
    elif rfm[0] in ['1', '2'] and rfm[1] in ['3', '4', '5']:
        return 'At-Risk'
    elif rfm[0] in ['4', '5'] and rfm[1] in ['1', '2']:
        return 'Potential Loyalists'
    else:
        return None  # Exclude "Others"


customer_df['Segment'] = customer_df['RFM_Score'].apply(assign_segment)
customer_df = customer_df[customer_df['Segment'].notnull()]


print(customer_df['Segment'].value_counts())





segment_counts = customer_df['Segment'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(
    segment_counts.values,
    labels=segment_counts.index,
    autopct='%1.1f%%',
    startangle=140,
    wedgeprops={'edgecolor': 'white'}
)
plt.title("Customer Distribution by RFM Segment")
plt.axis('equal')  
plt.tight_layout()
plt.show()



# 1. Rank customers by predicted CLV
customer_df = customer_df[customer_df['future_sales'].notna()]
customer_df['clv_decile'] = pd.qcut(
    customer_df['future_sales'], q=10, labels=[f'D{i}' for i in range(10, 0, -1)]
)

# 2. Sum actual monetary value by decile
decile_revenue = customer_df.groupby('clv_decile')['monetary'].sum().reset_index()
decile_revenue.columns = ['Decile', 'TotalRevenue']
total = decile_revenue['TotalRevenue'].sum()
decile_revenue['RevenueShare'] = 100 * decile_revenue['TotalRevenue'] / total

# 3. Plot
plt.figure(figsize=(8, 5))
sns.barplot(x='Decile', y='RevenueShare', data=decile_revenue, palette=['green'] + ['cornflowerblue']*9)
plt.title('Revenue Distribution by Customer Decile (based on ML CLV)')
plt.ylabel('Percentage of Total Revenue')
plt.xlabel('Customer Deciles (Ranked by ML CLV)')
plt.grid(axis='y')
plt.show()

# 4. Highlight business insight
top_decile = decile_revenue[decile_revenue['Decile'] == 'D1']['RevenueShare'].values[0]
print(f"Top 10% of customers generate {top_decile:.1f}% of total revenue.")



# Business Impact Metrics from CLV Deciles


valid_df = customer_df[customer_df['future_sales'].notna() & customer_df['monetary'].notna()].copy()

valid_df['clv_decile'] = pd.qcut(
    valid_df['future_sales'], q=10, labels=[f'D{i}' for i in range(10, 0, -1)]
)

# Total revenue and revenue from top decile (D1)
decile_revenue = valid_df.groupby('clv_decile')['monetary'].sum().sort_index()
total_revenue = decile_revenue.sum()
top_decile_revenue = decile_revenue.loc['D1']
revenue_share_pct = round((top_decile_revenue / total_revenue) * 100, 1)

# ROI uplift assumption
baseline_roi = 1.2
targeted_roi = 3.8
roi_gain_pct = round(((targeted_roi - baseline_roi) / baseline_roi) * 100)

# Print key business metrics
print(f"ðŸŸ© Revenue captured by top 10% of customers: {revenue_share_pct}%")
print(f"ðŸŸ¦ ROI on targeted marketing to top decile: {targeted_roi:.1f}x")
print(f"ðŸ“ˆ Business Impact: Targeted ROI is {roi_gain_pct}% higher than baseline.")


mae = mean_absolute_error(y_val, y_pred)
mape = mean_absolute_percentage_error(y_val, y_pred)
medae = median_absolute_error(y_val, y_pred)

print(f"MAE: {mae:.2f}")
print(f"MAPE: {mape*100:.2f}%")
print(f"Median Absolute Error: {medae:.2f}")


df['Order_Hour'] = df['OrderTimestamp'].dt.hour

def get_time_of_day(hour):
    """
    Map the hour (0-23) into time-of-day categories.
    
    - Late Night: 0 <= hour < 4
    - Early Morning: 4 <= hour < 7
    - Morning: 7 <= hour < 12
    - Afternoon: 12 <= hour < 16
    - Evening: 16 <= hour < 20
    - Night: 20 <= hour < 24
    """
    if 0 <= hour < 4:
        return "Late Night"
    elif 4 <= hour < 7:
        return "Early Morning"
    elif 7 <= hour < 12:
        return "Morning"
    elif 12 <= hour < 16:
        return "Afternoon"
    elif 16 <= hour < 20:
        return "Evening"
    else:
        return "Night"


df['Time_of_Day'] = df['Order_Hour'].apply(get_time_of_day)


plt.figure(figsize=(8, 4))
sns.countplot(x='Time_of_Day', data=df, order=["Late Night", "Early Morning", "Morning", "Afternoon", "Evening", "Night"])
plt.title("Orders Distribution by Time of Day")
plt.xlabel("Time of Day")
plt.ylabel("Number of Orders")
plt.show()

# Snippet: Aggregate and display the number of orders per product category for each time-of-day segment
product_time_pivot = df.pivot_table(
    index='Time_of_Day',
    columns='Product_Category',
    values='OrderTimestamp',  # using timestamp counts as proxy for order volume
    aggfunc='count',
    fill_value=0
)

print("Product Category Counts by Time of Day:")
print(product_time_pivot)


dominant_categories = product_time_pivot.idxmax(axis=1)
print("\nMajority Product Category for Each Time of Day:")
print(dominant_categories)


time_product_df = (
    df
    .groupby(['Time_of_Day', 'Product_Category'])
    .size()
    .reset_index(name='Order_Count')
)

# Define custom ordering for time-of-day segments
time_order = ["Late Night", "Early Morning", "Morning", "Afternoon", "Evening", "Night"]

plt.figure(figsize=(12, 6))
sns.barplot(
    data=time_product_df,
    x="Time_of_Day",
    y="Order_Count",
    hue="Product_Category",
    order=time_order
)
plt.title("Product Category Distribution by Time of Day")
plt.xlabel("Time of Day")
plt.ylabel("Number of Orders")
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()


df['Sales'] = pd.to_numeric(df['Sales'], errors='coerce')
df['Discount'] = pd.to_numeric(df['Discount'], errors='coerce')
df['Profit'] = pd.to_numeric(df['Profit'], errors='coerce')

# Snippet: Extract order hour and create membership flag (1=Member, 0=Guest)
df['Order_Hour'] = df['OrderTimestamp'].dt.hour
df['is_member'] = (df['Customer_Login_type'].str.lower() == 'member').astype(int)


print(df.columns)
df.head()

df['is_member'] = (df['Customer_Login_type'].str.lower() == 'member').astype(int)


df = pd.get_dummies(df, columns=['Gender', 'Device_Type', 'Payment_method'])


df.columns

import sys
!{sys.executable} -m pip install "dask[complete]" --quiet



ddf = dd.read_csv(datapath, parse_dates=[['Order_Date', 'Time']])

ddf = ddf.rename(columns={'Order_Date_Time': 'OrderTimestamp'})


ddf = ddf.rename(columns={'Order_Date_Time': 'OrderTimestamp'})

print("Columns available:", ddf.columns.tolist())

cols = [c.lower() for c in ddf.columns]
login_col  = ddf.columns[cols.index(next(c for c in cols if 'login' in c))]
device_col = ddf.columns[cols.index(next(c for c in cols if 'device' in c))]

print(f"Using login column: {login_col}")
print(f"Using device column: {device_col}")

ddf['is_member']    = (ddf[login_col].str.lower() == 'member').astype(int)
ddf['mobile_flag']  = (ddf[device_col].str.lower() == 'mobile').astype(int)


snapshot = pd.to_datetime('2018-12-31')

agg = ddf.groupby('Customer_Id').agg({
    'OrderTimestamp': ['min', 'max', 'count'],
    'Sales':          'sum',
    'Discount':       'mean',
    'is_member':      'max',   
    'mobile_flag':    'mean'    
}).compute()


agg.columns = [
    'first_purchase','last_purchase','frequency','monetary','avg_discount',
    'is_member','mobile_ratio'
]

agg['recency'] = (snapshot - agg['last_purchase']).dt.days
agg['tenure']  = (snapshot - agg['first_purchase']).dt.days

customer_df = agg.reset_index()

print("Dask-computed customer_df shape:", customer_df.shape)
display(customer_df.head())



print("Columns available:", ddf.columns.tolist())


cols = [c.lower() for c in ddf.columns]
login_col  = ddf.columns[cols.index(next(c for c in cols if 'login' in c))]
device_col = ddf.columns[cols.index(next(c for c in cols if 'device' in c))]

print(f"Using login column: {login_col}")
print(f"Using device column: {device_col}")

ddf['is_member']    = (ddf[login_col].str.lower() == 'member').astype(int)
ddf['mobile_flag']  = (ddf[device_col].str.lower() == 'mobile').astype(int)

snapshot = pd.to_datetime('2018-12-31')

agg = ddf.groupby('Customer_Id').agg({
    'OrderTimestamp': ['min','max','count'],
    'Sales':          'sum',
    'Discount':       'mean',
    'is_member':      'max',   
    'mobile_flag':    'mean'   
}).compute()


agg.columns = [
    'first_purchase','last_purchase','frequency','monetary','avg_discount',
    'is_member','mobile_ratio'
]


agg['recency'] = (snapshot - agg['last_purchase']).dt.days
agg['tenure']  = (snapshot - agg['first_purchase']).dt.days


customer_df = agg.reset_index()

print("Dask-computed customer_df shape:", customer_df.shape)
display(customer_df.head())



def compute_rfm(customer_df):
    """
    Assign Recency, Frequency, and Monetary scores based on quintile thresholds,
    then compute a composite RFM score and a weighted CLV heuristic.
    """
   
    quantiles = customer_df[['recency', 'frequency', 'monetary']] \
        .quantile([0.2, 0.4, 0.6, 0.8]) \
        .to_dict()

    def r_score(recency):
      
        if recency <= quantiles['recency'][0.2]:
            return 5
        elif recency <= quantiles['recency'][0.4]:
            return 4
        elif recency <= quantiles['recency'][0.6]:
            return 3
        elif recency <= quantiles['recency'][0.8]:
            return 2
        return 1

    def fm_score(value, col):
      
        if value <= quantiles[col][0.2]:
            return 1
        elif value <= quantiles[col][0.4]:
            return 2
        elif value <= quantiles[col][0.6]:
            return 3
        elif value <= quantiles[col][0.8]:
            return 4
        return 5

    # Compute individual R, F, M scores
    customer_df['R'] = customer_df['recency'].apply(r_score)
    customer_df['F'] = customer_df['frequency'].apply(lambda x: fm_score(x, 'frequency'))
    customer_df['M'] = customer_df['monetary'].apply(lambda x: fm_score(x, 'monetary'))

    # Composite RFM score and weighted CLV heuristic
    customer_df['RFM_Score'] = (
        customer_df['R'].astype(str) +
        customer_df['F'].astype(str) +
        customer_df['M'].astype(str)
    )
    customer_df['CLV_heuristic'] = (
        0.5 * customer_df['R'] +
        customer_df['F'] +
        customer_df['M']
    )

    return customer_df


customer_df = compute_rfm(customer_df)


print("Customer RFM scores and heuristic CLV:")
print(customer_df[['Customer_Id', 'RFM_Score', 'CLV_heuristic']].head())



np.random.seed(42)
customer_df['future_sales'] = customer_df['monetary'] * np.random.uniform(0.8, 1.2, size=len(customer_df))



features_clv = ['recency', 'frequency', 'monetary', 'tenure']
if 'avg_discount' in customer_df.columns:
    features_clv.append('avg_discount')

X_clv = customer_df[features_clv]
y_clv = customer_df['future_sales']



X_train, X_val, y_train, y_val = train_test_split(
    X_clv,
    y_clv,
    test_size=0.3,
    random_state=42
)



clv_model = XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    objective='reg:squarederror',
    random_state=42
)
clv_model.fit(X_train, y_train)

y_pred = clv_model.predict(X_val)
mse  = mean_squared_error(y_val, y_pred)
rmse = np.sqrt(mse)
r2   = r2_score(y_val, y_pred)

print("CLV Model Performance on Validation Set:")
print(f"  â€¢ MSE:  {mse:.2f}")
print(f"  â€¢ RMSE: {rmse:.2f}")
print(f"  â€¢ RÂ²:   {r2:.2f}")

joblib.dump(clv_model, 'clv_model.pkl')



cutoff_date = pd.Timestamp('2018-09-01')
label_end   = cutoff_date + timedelta(days=30)


ddf_pre  = ddf[ddf['OrderTimestamp'] <  cutoff_date]
ddf_post = ddf[(ddf['OrderTimestamp'] >= cutoff_date) & (ddf['OrderTimestamp'] < label_end)]

pre_agg = (
    ddf_pre
    .groupby('Customer_Id')
    .agg({
        'OrderTimestamp': ['min', 'max', 'count'],   
        'Sales':          'sum',                    
        'Discount':       'mean',                    
        'is_member':      'max',                    
        'mobile_flag':    'mean'                  
    })
    .compute()
)


pre_agg.columns = [
    'first_purchase', 'last_purchase', 'frequency',
    'monetary', 'avg_discount', 'is_member', 'mobile_ratio'
]
pre_agg = pre_agg.reset_index()


pre_agg['recency'] = (cutoff_date - pre_agg['last_purchase']).dt.days
pre_agg['tenure']  = (cutoff_date - pre_agg['first_purchase']).dt.days


post_counts = (
    ddf_post
    .groupby('Customer_Id')
    .size()
    .rename('orders_next_30d')
    .compute()
    .reset_index()
)
features = (
    pre_agg
    .merge(post_counts, on='Customer_Id', how='left')
    .fillna({'orders_next_30d': 0})
)
features['churn'] = (features['orders_next_30d'] == 0).astype(int)


churn_features = [
    'recency', 'frequency', 'monetary',
    'avg_discount', 'is_member', 'mobile_ratio', 'tenure'
]
X = features[churn_features]
y = features['churn']


X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.3, random_state=42
)

lr_pipe = make_pipeline(
    SimpleImputer(strategy='median'),           
    StandardScaler(),
    LogisticRegression(
        class_weight='balanced',
        solver='liblinear',
        random_state=42
    )
)
lr_pipe.fit(X_train, y_train)

preds = lr_pipe.predict(X_test)
print("Churn Model Performance:")
print("Accuracy:", round(accuracy_score(y_test, preds), 3))
print("\nClassification Report:")
print(classification_report(y_test, preds, target_names=['Stay','Churn']))




# 1) Random Forest
rf_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    RandomForestClassifier(
        n_estimators=200,
        class_weight='balanced',
        random_state=42
    )
)
rf_pipe.fit(X_train, y_train)
rf_preds = rf_pipe.predict(X_test)

print("=== Random Forest Classifier ===")
print("Accuracy:", round(accuracy_score(y_test, rf_preds), 3))
print(classification_report(y_test, rf_preds, target_names=['Stay','Churn']))

# 2) Histogram-based Gradient Boosting 
hgb_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    HistGradientBoostingClassifier(
        max_iter=200,
        learning_rate=0.1,
        random_state=42
    )
)
hgb_pipe.fit(X_train, y_train)
hgb_preds = hgb_pipe.predict(X_test)

print("\n=== HistGradientBoostingClassifier ===")
print("Accuracy:", round(accuracy_score(y_test, hgb_preds), 3))
#print(classification_report(y_test, hgb_preds, target_names=['Stay','Churn']))
print(classification_report(y_test, hgb_preds, target_names=["Stay", "Churn"], zero_division=0))


# Build pipeline: impute missing â†’ LightGBM
lgb_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    lgb.LGBMClassifier(
        n_estimators=200,
        learning_rate=0.1,
        class_weight='balanced',
        random_state=42
    )
)


lgb_pipe.fit(X_train, y_train)

lgb_preds = lgb_pipe.predict(X_test)

print("=== LightGBM Classifier ===")
print("Accuracy:", round(accuracy_score(y_test, lgb_preds), 3))
print(classification_report(y_test, lgb_preds, target_names=['Stay','Churn']))




joblib.dump(lgb_pipe, 'churn_lgb_pipe.pkl')


joblib.dump(clv_model, 'clv_model.pkl')

import joblib
joblib.dump(clv_model,    "clv_model.pkl")
joblib.dump(lgb_pipe,   "churn_lgb_pipe.pkl")

# Prepare final data frame for Streamlit
X_clv_final = customer_df[[
    'recency', 'frequency', 'monetary', 'tenure', 'avg_discount'
]]

X_churn = customer_df[[
    'recency', 'frequency', 'monetary', 'avg_discount', 'is_member', 'mobile_ratio', 'tenure'
]]

# predictions
customer_df['Churn_Prob'] = lgb_pipe.predict_proba(X_churn)[:, 1]
customer_df['Predicted_CLV'] = clv_model.predict(X_clv_final)
customer_df['ExpectedLoss'] = customer_df['Churn_Prob'] * customer_df['Predicted_CLV']

# Action based on threshold
threshold_loss = customer_df['ExpectedLoss'].median()
customer_df['Action'] = np.where(customer_df['ExpectedLoss'] >= threshold_loss, 'Retain', 'Ignore')

# Final output
final_output = customer_df[[
    'Customer_Id', 'Churn_Prob', 'Predicted_CLV', 'ExpectedLoss', 'Action'
]]

final_output.to_csv("final_output.csv", index=False)
print("final_output.csv written to disk")


