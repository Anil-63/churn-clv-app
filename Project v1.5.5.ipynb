# Snippet 1: Install (if needed) and import required libraries and modules
# -----------------------------------------------------------------------------
# This cell ensures all dependencies are installed, then loads them for:
#   • Data manipulation (pandas, NumPy)
#   • Visualization (Matplotlib, Seaborn)
#   • Modeling (scikit-learn, XGBoost, Prophet)
#   • Time-series forecasting (Prophet)
#   • Model persistence (joblib)
#   • Date/time utilities (datetime)
#
# Programmatic installation: attempts to import each package and installs via pip if missing.
import subprocess
import sys

packages = [
    "pandas", "numpy", "matplotlib", "seaborn",
    "scikit-learn", "xgboost", "prophet", "joblib"
]
for pkg in packages:
    try:
        __import__(pkg)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])

# Standard imports
import warnings
import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from datetime import datetime
from prophet import Prophet
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    mean_squared_error,
    r2_score,
    accuracy_score,
    classification_report,
)
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

datapath = ("/Users/anilkumar/Desktop/capestone/E-commerce_Dataset.csv")

df=pd.read_csv(datapath,parse_dates=[['Order_Date', 'Time']])

df.rename(columns={'Order_Date_Time': 'OrderTimestamp'}, inplace=True)
df.tail()

df.info()

df.describe()

df['Order_Day'] = df['OrderTimestamp'].dt.day_name()
df.head()

plt.figure(figsize=(8, 4))
sns.countplot(data=df, x='Order_Day', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
plt.title("Count of Orders by Day of the Week")
plt.xlabel("Day of Week")
plt.ylabel("Number of Orders")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8, 4))
sns.histplot(df['Sales'], bins=30,kde=True)
#A KDE is a non-parametric way to estimate the probability density function (PDF) of a random variable based on a finite data sample. Instead of displaying only discrete bars (the histogram), it provides a smooth curve that represents the distribution’s shape. This can help you better visualize the underlying probability distribution of the data.


plt.title("Distribution of Sales Amount")
plt.xlabel("Sales")
plt.ylabel("Frequency")
plt.show()

df['Order_Hour'] = df['OrderTimestamp'].dt.hour

#  Map an integer hour (0-23) into a time-of-day label
# Categorizes hours into one of six periods: Late Night, Early Morning, Morning, Afternoon, Evening, Night

def get_time_of_day(hour):
    """
    Map the hour (0-23) into time-of-day categories.
    
    - Late Night: 0 <= hour < 4
    - Early Morning: 4 <= hour < 7
    - Morning: 7 <= hour < 12
    - Afternoon: 12 <= hour < 16
    - Evening: 16 <= hour < 20
    - Night: 20 <= hour < 24
    """
    if 0 <= hour < 4:
        return "Late Night"
    elif 4 <= hour < 7:
        return "Early Morning"
    elif 7 <= hour < 12:
        return "Morning"
    elif 12 <= hour < 16:
        return "Afternoon"
    elif 16 <= hour < 20:
        return "Evening"
    else:
        return "Night"


df['Time_of_Day'] = df['Order_Hour'].apply(get_time_of_day)
#.apply(get_time_of_day) calls the function get_time_of_day on each individual value (each hour) in the "Order_Hour" column
#The apply() method iterates over every element in the column and passes it to the get_time_of_day function.

plt.figure(figsize=(8, 4))
sns.countplot(x='Time_of_Day', data=df, order=["Late Night", "Early Morning", "Morning", "Afternoon", "Evening", "Night"])
plt.title("Orders Distribution by Time of Day")
plt.xlabel("Time of Day")
plt.ylabel("Number of Orders")
plt.show()

# Snippet: Aggregate and display the number of orders per product category for each time-of-day segment
product_time_pivot = df.pivot_table(
    index='Time_of_Day',
    columns='Product_Category',
    values='OrderTimestamp',  # using timestamp counts as proxy for order volume
    aggfunc='count',
    fill_value=0
)

print("Product Category Counts by Time of Day:")
print(product_time_pivot)


dominant_categories = product_time_pivot.idxmax(axis=1)
print("\nMajority Product Category for Each Time of Day:")
print(dominant_categories)

# Plotting order counts by product category for each time-of-day segment
import seaborn as sns
import matplotlib.pyplot as plt

# Count orders per time segment and product category
time_product_df = (
    df
    .groupby(['Time_of_Day', 'Product_Category'])
    .size()
    .reset_index(name='Order_Count')
)

# Define custom ordering for time-of-day segments
time_order = ["Late Night", "Early Morning", "Morning", "Afternoon", "Evening", "Night"]

plt.figure(figsize=(12, 6))
sns.barplot(
    data=time_product_df,
    x="Time_of_Day",
    y="Order_Count",
    hue="Product_Category",
    order=time_order
)
plt.title("Product Category Distribution by Time of Day")
plt.xlabel("Time of Day")
plt.ylabel("Number of Orders")
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()


df['Sales'] = pd.to_numeric(df['Sales'], errors='coerce')
df['Discount'] = pd.to_numeric(df['Discount'], errors='coerce')
df['Profit'] = pd.to_numeric(df['Profit'], errors='coerce')

# Snippet: Extract order hour and create membership flag (1=Member, 0=Guest)
df['Order_Hour'] = df['OrderTimestamp'].dt.hour
df['is_member'] = (df['Customer_Login_type'].str.lower() == 'member').astype(int)


print(df.columns)
df.head()

df['is_member'] = (df['Customer_Login_type'].str.lower() == 'member').astype(int)


df = pd.get_dummies(df, columns=['Gender', 'Device_Type', 'Payment_method'])


df.columns

import sys
!{sys.executable} -m pip install "dask[complete]" --quiet

import dask.dataframe as dd
import pandas as pd

# 1) Load the CSV via Dask
ddf = dd.read_csv(
    "/Users/anilkumar/Desktop/capestone/E-commerce_Dataset.csv",
    parse_dates=[['Order_Date', 'Time']],
    assume_missing=True
)

# Rename the combined datetime column
ddf = ddf.rename(columns={'Order_Date_Time': 'OrderTimestamp'})

# 2) Inspect column names to find the exact login/device columns
print("Columns available:", ddf.columns.tolist())

# 3) Identify the login-type and device-type columns dynamically
cols = [c.lower() for c in ddf.columns]
login_col  = ddf.columns[cols.index(next(c for c in cols if 'login' in c))]
device_col = ddf.columns[cols.index(next(c for c in cols if 'device' in c))]

print(f"Using login column: {login_col}")
print(f"Using device column: {device_col}")

# 4) Create flags
ddf['is_member']    = (ddf[login_col].str.lower() == 'member').astype(int)
ddf['mobile_flag']  = (ddf[device_col].str.lower() == 'mobile').astype(int)

# 5) Perform the groupby with all required aggregates
snapshot = pd.to_datetime('2018-12-31')

agg = ddf.groupby('Customer_Id').agg({
    'OrderTimestamp': ['min', 'max', 'count'],
    'Sales':          'sum',
    'Discount':       'mean',
    'is_member':      'max',    # did they ever log in as a member?
    'mobile_flag':    'mean'    # proportion of orders on mobile
}).compute()

# 6) Flatten and rename columns
agg.columns = [
    'first_purchase','last_purchase','frequency','monetary','avg_discount',
    'is_member','mobile_ratio'
]

# 7) Compute recency & tenure
agg['recency'] = (snapshot - agg['last_purchase']).dt.days
agg['tenure']  = (snapshot - agg['first_purchase']).dt.days

# 8) Reset index for a clean Pandas DataFrame
customer_df = agg.reset_index()

print("Dask-computed customer_df shape:", customer_df.shape)
display(customer_df.head())


pip install "pyarrow>=10.0.1"


# Compute customer-level metrics (recency, frequency, monetary, member status, mobile ratio) using Dask
import dask.dataframe as dd
import pandas as pd

# 1) Load the CSV via Dask
ddf = dd.read_csv(
    "/Users/anilkumar/Desktop/capestone/E-commerce_Dataset.csv",
    parse_dates=[['Order_Date','Time']],
    assume_missing=True
).rename(columns={'Order_Date_Time': 'OrderTimestamp'})

# 2) Inspect column names to find the exact login/device columns
print("Columns available:", ddf.columns.tolist())

# 3) Identify the login-type and device-type columns dynamically
cols = [c.lower() for c in ddf.columns]
login_col  = ddf.columns[cols.index(next(c for c in cols if 'login' in c))]
device_col = ddf.columns[cols.index(next(c for c in cols if 'device' in c))]

print(f"Using login column: {login_col}")
print(f"Using device column: {device_col}")

# 4) Create flags
ddf['is_member']    = (ddf[login_col].str.lower() == 'member').astype(int)
ddf['mobile_flag']  = (ddf[device_col].str.lower() == 'mobile').astype(int)

# 5) Perform the groupby with all required aggregates
snapshot = pd.to_datetime('2018-12-31')

agg = ddf.groupby('Customer_Id').agg({
    'OrderTimestamp': ['min','max','count'],
    'Sales':          'sum',
    'Discount':       'mean',
    'is_member':      'max',   # did they ever log in as a member?
    'mobile_flag':    'mean'   # proportion of orders on mobile
}).compute()

# 6) Flatten and rename columns
agg.columns = [
    'first_purchase','last_purchase','frequency','monetary','avg_discount',
    'is_member','mobile_ratio'
]

# 7) Compute recency & tenure
agg['recency'] = (snapshot - agg['last_purchase']).dt.days
agg['tenure']  = (snapshot - agg['first_purchase']).dt.days

# 8) Reset index for a clean Pandas DataFrame
customer_df = agg.reset_index()

print("Dask-computed customer_df shape:", customer_df.shape)
display(customer_df.head())


# Snippet: Calculate RFM scores and a heuristic CLV for each customer
def compute_rfm(customer_df):
    """
    Assign Recency, Frequency, and Monetary scores based on quintile thresholds,
    then compute a composite RFM score and a weighted CLV heuristic.
    """
    # Determine quintile thresholds for recency, frequency, and monetary
    quantiles = customer_df[['recency', 'frequency', 'monetary']] \
        .quantile([0.2, 0.4, 0.6, 0.8]) \
        .to_dict()

    def r_score(recency):
        # Higher score for more recent purchases
        if recency <= quantiles['recency'][0.2]:
            return 5
        elif recency <= quantiles['recency'][0.4]:
            return 4
        elif recency <= quantiles['recency'][0.6]:
            return 3
        elif recency <= quantiles['recency'][0.8]:
            return 2
        return 1

    def fm_score(value, col):
        # Higher score for greater frequency or monetary value
        if value <= quantiles[col][0.2]:
            return 1
        elif value <= quantiles[col][0.4]:
            return 2
        elif value <= quantiles[col][0.6]:
            return 3
        elif value <= quantiles[col][0.8]:
            return 4
        return 5

    # Compute individual R, F, M scores
    customer_df['R'] = customer_df['recency'].apply(r_score)
    customer_df['F'] = customer_df['frequency'].apply(lambda x: fm_score(x, 'frequency'))
    customer_df['M'] = customer_df['monetary'].apply(lambda x: fm_score(x, 'monetary'))

    # Composite RFM score and weighted CLV heuristic
    customer_df['RFM_Score'] = (
        customer_df['R'].astype(str) +
        customer_df['F'].astype(str) +
        customer_df['M'].astype(str)
    )
    customer_df['CLV_heuristic'] = (
        0.5 * customer_df['R'] +
        customer_df['F'] +
        customer_df['M']
    )

    return customer_df

# Apply RFM and CLV computation
customer_df = compute_rfm(customer_df)

# Display the first few results
print("Customer RFM scores and heuristic CLV:")
print(customer_df[['Customer_Id', 'RFM_Score', 'CLV_heuristic']].head())


# Simulate a future_sales target by adding random noise to historical monetary values
np.random.seed(42)
customer_df['future_sales'] = customer_df['monetary'] * np.random.uniform(0.8, 1.2, size=len(customer_df))


# CELL 6.1: Prepare features and target for CLV modeling
from sklearn.model_selection import train_test_split
import numpy as np

# Define feature columns (include avg_discount if present)
features_clv = ['recency', 'frequency', 'monetary', 'tenure']
if 'avg_discount' in customer_df.columns:
    features_clv.append('avg_discount')

X_clv = customer_df[features_clv]
y_clv = customer_df['future_sales']


# CELL 6.2: Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(
    X_clv,
    y_clv,
    test_size=0.3,
    random_state=42
)


# CELL 6.3: Train XGBoost, evaluate performance, and save the model
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib

# Instantiate and fit the regressor
clv_model = XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    objective='reg:squarederror',
    random_state=42
)
clv_model.fit(X_train, y_train)

# Predict on validation set and compute metrics
y_pred = clv_model.predict(X_val)
mse  = mean_squared_error(y_val, y_pred)
rmse = np.sqrt(mse)
r2   = r2_score(y_val, y_pred)

print("CLV Model Performance on Validation Set:")
print(f"  • MSE:  {mse:.2f}")
print(f"  • RMSE: {rmse:.2f}")
print(f"  • R²:   {r2:.2f}")

# Save the trained model for later use
joblib.dump(clv_model, 'clv_model.pkl')


# ================================
# CELL X: Churn Modeling (Logistic Regression) using Dask‐computed features
# ================================
import pandas as pd
from datetime import timedelta
from sklearn.model_selection import train_test_split
from sklearn.pipeline        import make_pipeline
from sklearn.impute          import SimpleImputer
from sklearn.preprocessing   import StandardScaler
from sklearn.linear_model    import LogisticRegression
from sklearn.metrics         import accuracy_score, classification_report

# 1) Define cutoff and label windows
cutoff_date = pd.Timestamp('2018-09-01')
label_end   = cutoff_date + timedelta(days=30)

# 2) Split the Dask DataFrame into pre‐ and post‐periods
ddf_pre  = ddf[ddf['OrderTimestamp'] <  cutoff_date]
ddf_post = ddf[(ddf['OrderTimestamp'] >= cutoff_date) & (ddf['OrderTimestamp'] < label_end)]

# 3) Aggregate pre‐period features per customer
pre_agg = (
    ddf_pre
    .groupby('Customer_Id')
    .agg({
        'OrderTimestamp': ['min', 'max', 'count'],   # first/last purchase, frequency
        'Sales':          'sum',                     # monetary
        'Discount':       'mean',                    # avg_discount
        'is_member':      'max',                     # ever a member
        'mobile_flag':    'mean'                     # proportion of mobile orders
    })
    .compute()
)

# 4) Flatten MultiIndex and rename
pre_agg.columns = [
    'first_purchase', 'last_purchase', 'frequency',
    'monetary', 'avg_discount', 'is_member', 'mobile_ratio'
]
pre_agg = pre_agg.reset_index()

# 5) Compute recency & tenure relative to cutoff_date
pre_agg['recency'] = (cutoff_date - pre_agg['last_purchase']).dt.days
pre_agg['tenure']  = (cutoff_date - pre_agg['first_purchase']).dt.days

# 6) Label churn: customers with zero orders in the 30‐day label window
post_counts = (
    ddf_post
    .groupby('Customer_Id')
    .size()
    .rename('orders_next_30d')
    .compute()
    .reset_index()
)
features = (
    pre_agg
    .merge(post_counts, on='Customer_Id', how='left')
    .fillna({'orders_next_30d': 0})
)
features['churn'] = (features['orders_next_30d'] == 0).astype(int)

# 7) Prepare feature matrix X and target y
churn_features = [
    'recency', 'frequency', 'monetary',
    'avg_discount', 'is_member', 'mobile_ratio', 'tenure'
]
X = features[churn_features]
y = features['churn']

# 8) Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.3, random_state=42
)

# 9) Build pipeline with imputation, scaling, and logistic regression
lr_pipe = make_pipeline(
    SimpleImputer(strategy='median'),           # fill any NaNs with column medians
    StandardScaler(),
    LogisticRegression(
        class_weight='balanced',
        solver='liblinear',
        random_state=42
    )
)
lr_pipe.fit(X_train, y_train)

# 10) Predict and evaluate
preds = lr_pipe.predict(X_test)
print("Churn Model Performance:")
print("Accuracy:", round(accuracy_score(y_test, preds), 3))
print("\nClassification Report:")
print(classification_report(y_test, preds, target_names=['Stay','Churn']))




# ===============================
# CELL X+1: Try Random Forest & HistGradientBoostingClassifier
# ===============================
from sklearn.pipeline            import make_pipeline
from sklearn.impute              import SimpleImputer
from sklearn.ensemble            import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.metrics             import accuracy_score, classification_report

# 1) Random Forest
rf_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    RandomForestClassifier(
        n_estimators=200,
        class_weight='balanced',
        random_state=42
    )
)
rf_pipe.fit(X_train, y_train)
rf_preds = rf_pipe.predict(X_test)

print("=== Random Forest Classifier ===")
print("Accuracy:", round(accuracy_score(y_test, rf_preds), 3))
print(classification_report(y_test, rf_preds, target_names=['Stay','Churn']))

# 2) Histogram-based Gradient Boosting (fixed)
hgb_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    HistGradientBoostingClassifier(
        max_iter=200,
        learning_rate=0.1,
        random_state=42
    )
)
hgb_pipe.fit(X_train, y_train)
hgb_preds = hgb_pipe.predict(X_test)

print("\n=== HistGradientBoostingClassifier ===")
print("Accuracy:", round(accuracy_score(y_test, hgb_preds), 3))
print(classification_report(y_test, hgb_preds, target_names=['Stay','Churn']))


# ===============================
# CELL X+5: Install & Run LightGBM Classifier for Churn
# ===============================
import sys
import subprocess

# 1) Install LightGBM if it's not already present
subprocess.check_call([sys.executable, "-m", "pip", "install", "lightgbm"])

# 2) Now import and run the model
import lightgbm as lgb
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report

# Build pipeline: impute missing → LightGBM
lgb_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    lgb.LGBMClassifier(
        n_estimators=200,
        learning_rate=0.1,
        class_weight='balanced',
        random_state=42
    )
)

# Fit on the training data
lgb_pipe.fit(X_train, y_train)

# Predict on the test set
lgb_preds = lgb_pipe.predict(X_test)

# Evaluate
print("=== LightGBM Classifier ===")
print("Accuracy:", round(accuracy_score(y_test, lgb_preds), 3))
print(classification_report(y_test, lgb_preds, target_names=['Stay','Churn']))




joblib.dump(lgb_pipe, 'churn_lgb_pipe.pkl')


joblib.dump(clv_model, 'clv_model.pkl')

import joblib
joblib.dump(clv_model,    "clv_model.pkl")
joblib.dump(lgb_pipe,   "churn_lgb_pipe.pkl")

# ✅ Prepare final data frame for Streamlit
X_clv_final = customer_df[[
    'recency', 'frequency', 'monetary', 'tenure', 'avg_discount'
]]

X_churn = customer_df[[
    'recency', 'frequency', 'monetary', 'avg_discount', 'is_member', 'mobile_ratio', 'tenure'
]]

# Make predictions
customer_df['Churn_Prob'] = lgb_pipe.predict_proba(X_churn)[:, 1]
customer_df['Predicted_CLV'] = clv_model.predict(X_clv_final)
customer_df['ExpectedLoss'] = customer_df['Churn_Prob'] * customer_df['Predicted_CLV']

# Action based on threshold
threshold_loss = customer_df['ExpectedLoss'].median()
customer_df['Action'] = np.where(customer_df['ExpectedLoss'] >= threshold_loss, 'Retain', 'Ignore')

# Final output
final_output = customer_df[[
    'Customer_Id', 'Churn_Prob', 'Predicted_CLV', 'ExpectedLoss', 'Action'
]]

final_output.to_csv("final_output.csv", index=False)
print("✅ final_output.csv written to disk")




